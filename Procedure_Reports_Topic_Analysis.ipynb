{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Extract Impression attribute\n",
    "def extract_impression(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = re.search(r'impression:\\s*(.*)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "# Remove punctuations and digits\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove digits and punctuation\n",
    "    text = re.sub(r'[\\d' + string.punctuation + ']', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Load data from CSV containing procedure reports\n",
    "df = pd.read_csv(\" \", sep=',')\n",
    "\n",
    "# Extract impression and clean text\n",
    "df['impression_extracted'] = df['procedure_report'].apply(extract_impression)\n",
    "df['impression_cleaned'] = df['impression_extracted'].apply(clean_text)\n",
    "\n",
    "# Drop rows with no Impression attribute\n",
    "df = df.dropna(subset=['impression_cleaned'])\n",
    "\n",
    "# Preview results\n",
    "print(df[['impression_extracted', 'impression_cleaned']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Negation Detection\n",
    "import scispacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import termset\n",
    "\n",
    "# Load SciSpaCy model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Set up negation termset\n",
    "ts = termset(\"en_clinical\")\n",
    "ts.add_patterns({\n",
    "    \"preceding_negations\": [\"unable\"],\n",
    "    \"following_negations\": [\"was negative\"]\n",
    "})\n",
    "\n",
    "# Add negex component to pipeline\n",
    "if \"negex\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"negex\", config={\"neg_termset\": ts.get_patterns()})\n",
    "\n",
    "# Apply negation-aware processing\n",
    "def process_impression(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    doc = nlp(text)\n",
    "\n",
    "    phrases = []\n",
    "    for ent in doc.ents:\n",
    "        ent_text = ent.text.strip().replace(\" \", \"_\")\n",
    "        if ent._.negex:\n",
    "            phrases.append(f\"no_{ent_text}\")\n",
    "        else:\n",
    "            phrases.append(ent_text)\n",
    "    return \" \".join(phrases) if phrases else None\n",
    "\n",
    "# Apply to DataFrame\n",
    "df[\"impression_negex\"] = df[\"impression_cleaned\"].apply(process_impression)\n",
    "\n",
    "# Print result\n",
    "print(df[[\"impression_cleaned\", \"impression_negex\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61989971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords and generic words\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Extend stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([\n",
    "    'personalname', 'alphanumericid', 'examination', 'preliminary', 'impression', \n",
    "    'chest', 'view', 'change', 'final', 'available', 'evidence', 'see', 'right', \n",
    "    'stable', 'date', 'compare', 'interpretation', 'portable', 'lung', 'leave', \n",
    "    'report', 'process', 'review', 'resident', 'radiologist', 'attend', \n",
    "    'finding', 'findings', 'electronic', 'and', 'sign'\n",
    "])\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords_custom(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    tokens = text.lower().split()\n",
    "    filtered = [t for t in tokens if t not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# Apply stopword removal\n",
    "df['impression_cleaned'] = df['impression_negex'].apply(remove_stopwords_custom)\n",
    "\n",
    "# Remove rows with empty cleaned impressions\n",
    "df = df[df['impression_cleaned'].str.strip() != ''].reset_index(drop=True)\n",
    "\n",
    "# Show final result\n",
    "print(df[['impression_negex', 'impression_cleaned']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb113db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Tokenize cleaned impression strings\n",
    "tokenized_data = df['impression_cleaned'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Remove empty lists\n",
    "filtered_list = [doc for doc in tokenized_data if doc]\n",
    "\n",
    "# Create a dictionary\n",
    "dictionary = corpora.Dictionary(filtered_list)\n",
    "\n",
    "# Create a Bag-of-Words (BoW) representation\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in filtered_list]\n",
    "\n",
    "# Create a TF-IDF model\n",
    "tfidf_model = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Apply the model to get the TF-IDF corpus\n",
    "corpus_tfidf = tfidf_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86651bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal no. of topics using Coherence score\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=gensim.models.LdaMulticore(corpus_tfidf, num_topics=num_topics, id2word=dictionary,random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=filtered_list, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus_tfidf, texts=filtered_list, start=2, limit=40, step=1)\n",
    "limit=40; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0974dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate topics based on the no. of optimal topics which is '5' in this case \n",
    "lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary,random_state=0)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0292b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dominant topic in each sentence of the text\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=filtered_list):\n",
    "    # Init output\n",
    "    sent_topics_list = []\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        if row:\n",
    "            topic_num, prop_topic = row[0]  # Dominant topic\n",
    "            wp = ldamodel.show_topic(topic_num)\n",
    "            topic_keywords = \", \".join([word for word, prop in wp])\n",
    "            sent_topics_list.append([int(topic_num), round(prop_topic, 4), topic_keywords])\n",
    "    \n",
    "    sent_topics_df = pd.DataFrame(sent_topics_list, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords'])\n",
    "    \n",
    "    # Add original text to the end of the output\n",
    "    sent_topics_df = pd.concat([sent_topics_df, pd.Series(texts, name='Text')], axis=1)\n",
    "    \n",
    "    return sent_topics_df\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=filtered_list)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of documents per topic\n",
    "topic_counts = df_dominant_topic['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of documents per topic\n",
    "topic_percentages = topic_counts / len(df_dominant_topic) * 100\n",
    "\n",
    "# Combine into a single DataFrame for better readability\n",
    "topic_distribution = pd.DataFrame({\n",
    "    'Topic': topic_counts.index,\n",
    "    'Num_Documents': topic_counts.values,\n",
    "    'Percentage': topic_percentages.values.round(2)\n",
    "})\n",
    "\n",
    "# Optionally sort by percentage\n",
    "topic_distribution = topic_distribution.sort_values(by='Percentage', ascending=False)\n",
    "\n",
    "print(topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic analysis using chi-square analysis\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim import corpora, models\n",
    "\n",
    "num_docs = len(corpus_tfidf)\n",
    "num_terms = len(dictionary)\n",
    "dense_matrix = np.zeros((num_docs, num_terms))\n",
    "\n",
    "for i, doc in enumerate(corpus_tfidf):\n",
    "    for term_id, tfidf_value in doc:\n",
    "        dense_matrix[i, term_id] = tfidf_value\n",
    "\n",
    "# Label encode dominant topics for supervised feature selection\n",
    "labels = df_dominant_topic['Dominant_Topic'].values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Apply chi-square feature selection to find top 20 TF-IDF features\n",
    "chi2_selector = SelectKBest(score_func=chi2, k=20)\n",
    "X_chi2 = chi2_selector.fit_transform(dense_matrix, y_encoded)\n",
    "\n",
    "# Get selected term indices and actual terms from dictionary\n",
    "selected_term_ids = chi2_selector.get_support(indices=True)\n",
    "top_features = [dictionary[i] for i in selected_term_ids]\n",
    "\n",
    "print(\"Top 20 chi-square selected features:\")\n",
    "print(top_features)\n",
    "\n",
    "# For each topic, count documents containing each selected feature\n",
    "binary_matrix = (dense_matrix[:, selected_term_ids] > 0).astype(int)\n",
    "topic_feature_counts = {feature: [0] * len(label_encoder.classes_) for feature in top_features}\n",
    "\n",
    "for i in range(num_docs):\n",
    "    topic_id = y_encoded[i]\n",
    "    present_features = binary_matrix[i]\n",
    "    for j, present in enumerate(present_features):\n",
    "        if present:\n",
    "            feature = top_features[j]\n",
    "            topic_feature_counts[feature][topic_id] += 1\n",
    "\n",
    "# Sum feature counts across top 20 for each topic\n",
    "topic_total_counts = np.zeros(len(label_encoder.classes_))\n",
    "for feature in top_features:\n",
    "    topic_total_counts += topic_feature_counts[feature]\n",
    "\n",
    "# Find topic with highest total counts\n",
    "best_topic_index = int(np.argmax(topic_total_counts))\n",
    "best_topic = label_encoder.inverse_transform([best_topic_index])[0]\n",
    "\n",
    "print(f\"\\nTopic most associated with comorbidity-related features: {best_topic}\")\n",
    "print(f\"Total relevant feature occurrences in that topic: {int(topic_total_counts[best_topic_index])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores and indices of selected features\n",
    "chi2_scores = chi2_selector.scores_[selected_term_ids]\n",
    "\n",
    "# Format feature names with chi-square value\n",
    "formatted_features = [\n",
    "    f\"{dictionary[term_id]} (χ²({len(label_encoder.classes_)-1}) = {score:.1f})\"\n",
    "    for term_id, score in zip(selected_term_ids, chi2_scores)\n",
    "]\n",
    "\n",
    "# Use existing topic_feature_counts to build final DataFrame\n",
    "feature_topic_df = pd.DataFrame(\n",
    "    topic_feature_counts, \n",
    "    index=[f'Topic_{i}' for i in range(len(label_encoder.classes_))]\n",
    ").T  # Transpose\n",
    "\n",
    "# Rename index to include chi-square values\n",
    "feature_topic_df.index = formatted_features\n",
    "\n",
    "# Sort columns by topic number (optional)\n",
    "feature_topic_df.columns = sorted(feature_topic_df.columns, key=lambda x: int(x.split(\"_\")[1]))\n",
    "\n",
    "# Display\n",
    "print(\"Number of clinical notes containing each top feature per topic:\\n\")\n",
    "print(feature_topic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b1ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
